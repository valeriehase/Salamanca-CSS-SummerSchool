{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install and load all necessary packages"
      ],
      "metadata": {
        "id": "VLE3XL2HHC0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures\n",
        "import spacy\n",
        "\n",
        "# Ensure you have the necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "MbI9v6RoHlxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Getting Text into R/Python"
      ],
      "metadata": {
        "id": "DAuS1BLsHHtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We load data (a csv-file with ratings and content of TV series) from the Github repository\n",
        "url = \"https://raw.githubusercontent.com/valeriehase/Salamanca-CSS-SummerSchool/main/Processing%20text%20and%20text%20as%20data/data_tvseries.csv\"\n",
        "data = pd.read_csv(url, sep = \";\")"
      ],
      "metadata": {
        "id": "W1-_iZLLHI65"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8i2WxylG-w7"
      },
      "outputs": [],
      "source": [
        "#Check data by inspecting first rows via head()\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect data in variable \"Year\" for first observation - any issues?\n",
        "data.iloc[0, 1]"
      ],
      "metadata": {
        "id": "maeFCGobYM7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Cleaning/Normalizing Text\n",
        "\n"
      ],
      "metadata": {
        "id": "zRjzaX8lhRbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning Text via Regular Expressions\n",
        "\n"
      ],
      "metadata": {
        "id": "jomUJqMumBbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's remove the number, point and blank space before the TV series in our\n",
        "# variable \"Title\" using replace()\n",
        "data[\"Title\"] = data[\"Title\"].replace(\"^[0-9]+\\.\", \"\", regex = True)\n",
        "\n",
        "#Inspect the result\n",
        "data.head()"
      ],
      "metadata": {
        "id": "m_CChgRaGwep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ok, let's have some fun with this.\n",
        "# Using the str.contains() function, we identify all TV series\n",
        "# that contain the word \"drama\" in the variable \"Description\".\n",
        "data[data[\"Description\"].str.contains(\"[D|d]rama\")].head()"
      ],
      "metadata": {
        "id": "lxBGnFU1pnS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's get all observations that contain the word\n",
        "# \"drama\" or the word \"crime\" in the variable \"Description\"\n",
        "data[data[\"Description\"].str.contains(\"[D|d]rama|[C|c]rime\")].head()"
      ],
      "metadata": {
        "id": "dIGPurCT75j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your turn!\n",
        "# Can you identify all series that play in Spain?\n",
        "data[data[\"Description\"].str.contains(\"in Spain\")]"
      ],
      "metadata": {
        "id": "Youi2Znz_dQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your turn!\n",
        "# Can you identify all series that deal with superheroes # and replace the term \"superhero/superheroes in the variable \"Description\"\n",
        "# with \"fancy Python programmers\"?\n",
        "data[\"Description\"].str.replace(\"[S|s]uperhero[es]* \", \"fancy Python programmers\", regex = True).head()"
      ],
      "metadata": {
        "id": "9l8uQm1cAgvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizing Text\n",
        "\n"
      ],
      "metadata": {
        "id": "JkrueTT1JCbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the stop words and stemmer\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "#Write a function that contains all necessary preprocessing steps\n",
        "def clean_description(description):\n",
        "    # Tokenize the description\n",
        "    words = word_tokenize(description)\n",
        "    # Remove special signs and convert to lower case\n",
        "    words = [word.lower() for word in words if word.isalpha()]\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    # Apply stemming\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    return words\n",
        "\n",
        "tokens = [clean_description(description) for description in data[\"Description\"]]"
      ],
      "metadata": {
        "id": "-PAqjW8SLWBK"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Look at original first text\n",
        "data[\"Description\"].iloc[0]"
      ],
      "metadata": {
        "id": "cCtasOkVH20u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Look at preprocessed first text\n",
        "tokens[0]"
      ],
      "metadata": {
        "id": "6pdhJBhTHDfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your turn!\n",
        "# Can you create a list of 3-5 stop words that you think are unique to this corpus\n",
        "# and remove these as part of the existing preprocessing pipeline?\n",
        "unique_stopwords = [\"one\", \"two\", \"three\", \"four\", \"five\"]\n",
        "\n",
        "#Write a function that contains all necessary preprocessing steps\n",
        "def clean_description(description):\n",
        "    # Tokenize the description\n",
        "    words = word_tokenize(description)\n",
        "    # Remove special signs and convert to lower case\n",
        "    words = [word.lower() for word in words if word.isalpha()]\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    # Remove unique list of stopwords\n",
        "    words = [word for word in words if word not in unique_stopwords]\n",
        "    # Apply stemming\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    return words\n",
        "\n",
        "tokens = [clean_description(description) for description in data[\"Description\"]]"
      ],
      "metadata": {
        "id": "x334neiOPxIn"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text-as-Data Representations\n",
        "\n"
      ],
      "metadata": {
        "id": "hIdETE9Qf9B6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag-of-words approach: Document-feature matrix"
      ],
      "metadata": {
        "id": "JGrXU340gGLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a new dfm function that contains all necessary preprocessing steps\n",
        "def clean_description_dfm(description):\n",
        "    # Tokenize the description\n",
        "    words = word_tokenize(description)\n",
        "    # Remove special signs and convert to lower case\n",
        "    words = [word.lower() for word in words if word.isalpha()]\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    # Apply stemming\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    #Additionally re-join as string\n",
        "    return ' '.join(words)  # Join the tokens back into a single string\n",
        "\n",
        "tokens_dfm = [clean_description_dfm(description) for description in data[\"Description\"]]\n",
        "\n",
        "#Create a document-feature matrix\n",
        "vectorizer = CountVectorizer()\n",
        "dfm = vectorizer.fit_transform(tokens_dfm)\n",
        "\n",
        "#print the result in dense format\n",
        "pd.DataFrame(dfm.todense(), columns = vectorizer.get_feature_names_out()).head()"
      ],
      "metadata": {
        "id": "uVjq0BVYlvcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert dfm to a dense format for calculation\n",
        "dfm_dense = dfm.toarray()\n",
        "\n",
        "# Get feature names\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "#Check most frequent features\n",
        "def top_features(matrix, feature_names, top_n):\n",
        "    # Sum the occurrences of each feature\n",
        "    feature_sums = np.sum(matrix, axis = 0)\n",
        "    # Create a data frame to hold feature names and their corresponding sums\n",
        "    feature_sums_df = pd.DataFrame({'feature': feature_names, 'count': feature_sums})\n",
        "    # Sort the data frame by count in descending order and get the top N features\n",
        "    top_features_df = feature_sums_df.sort_values(by = \"count\", ascending = False).head(top_n)\n",
        "    return top_features_df\n",
        "\n",
        "topfeatures = top_features(dfm_dense, feature_names, 10)\n",
        "\n",
        "topfeatures"
      ],
      "metadata": {
        "id": "9vLNlsQii-jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize results with a word cloud\n",
        "\n",
        "#get feature sums\n",
        "feature_sums = np.sum(dfm_dense, axis=0)\n",
        "\n",
        "# Create a dictionary of features and their corresponding sums\n",
        "feature_counts = dict(zip(feature_names, feature_sums))\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud = WordCloud(max_words = 100, background_color = \"white\").generate_from_frequencies(feature_counts)\n",
        "\n",
        "# Display the word cloud using matplotlib\n",
        "plt.figure(figsize = (10, 5))\n",
        "plt.imshow(wordcloud, interpolation = \"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jbkc0iPWpqSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beyond bag-of-words: Ngrams"
      ],
      "metadata": {
        "id": "y8kcemw7uZpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the list of lists into a single list of tokens\n",
        "all_tokens = [token for sublist in tokens for token in sublist]\n",
        "\n",
        "# Find bigram collocations\n",
        "finder = BigramCollocationFinder.from_words(all_tokens)\n",
        "\n",
        "# Filter out bigrams that occur less than 10 times\n",
        "finder.apply_freq_filter(10)\n",
        "\n",
        "# Score the bigrams using the likelihood ratio\n",
        "scored = finder.score_ngrams(BigramAssocMeasures.likelihood_ratio)\n",
        "\n",
        "# Convert to a DataFrame for easier manipulation\n",
        "scored_df = pd.DataFrame(scored, columns = [\"bigram\", \"likelihood_ratio\"])\n",
        "\n",
        "# Sort by the likelihood ratio in descending order and take the top 10\n",
        "top_10_collocations = scored_df.sort_values(by = \"likelihood_ratio\", ascending=False).head(10)\n",
        "\n",
        "# Print the top 10 collocations\n",
        "top_10_collocations"
      ],
      "metadata": {
        "id": "Ho6PfXkSuYnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beyond bag-of-words: Part-of-speech tagging"
      ],
      "metadata": {
        "id": "N4u551k31Cbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For simplicity, run for fewer documents\n",
        "sample = data.head(1)\n",
        "\n",
        "# Part-of-speech tagging, include only related variables\n",
        "pos_tags = []\n",
        "for idx, row in sample.iterrows():\n",
        "    doc = nlp(row[\"Description\"])\n",
        "    for sent in doc.sents:\n",
        "        for token in sent:\n",
        "            pos_tags.append({\n",
        "                'sentence_id': sent.start,\n",
        "                'token_id': token.i,\n",
        "                'token': token.text,\n",
        "                'upos': token.pos_\n",
        "            })\n",
        "\n",
        "# Convert the list of dictionaries to a DataFrame\n",
        "pos_df = pd.DataFrame(pos_tags)\n",
        "\n",
        "# Display the first 10 rows\n",
        "pos_df.head(10)"
      ],
      "metadata": {
        "id": "ZgVq1WQU1Fhv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}