{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install and load all necessary packages"
      ],
      "metadata": {
        "id": "4F1FMDKl1l7l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "54SvWof7XGwM"
      },
      "outputs": [],
      "source": [
        "# Usually, we would install packages like this - but this takes forever on Colab notebooks (at least 15 min.)\n",
        "# install.packages(\"dplyr\")\n",
        "# install.packages(\"quanteda\")\n",
        "# install.packages(\"RCurl\")\n",
        "# install.packages(\"quanteda.textplots\")\n",
        "# install.packages(\"quanteda.textstats\")\n",
        "# install.packages(\"udpipe\")\n",
        "\n",
        "# For this session, we therefore \"only\" load an already compiled, zipped file with all R packages\n",
        "# This speeds up the installation process (but is a \"turnaround\")\n",
        "\n",
        "# create a folder called \"library\"\n",
        "system(\"mkdir library\")\n",
        "\n",
        "# download the R environment file containing complied packages\n",
        "R_environment_file <- \"https://drive.usercontent.google.com/download?id=1vmeZC68FTNNyanEl3c6DRWOvjumE1RMu&export=download&authuser=0&confirm=t&uuid=843db779-c069-4811-a326-2a9847eb1bbc&at=APZUnTUXVHG1k7a4evb5pcCJ3XQc:1717563767799\"\n",
        "download.file(R_environment_file, destfile=\"./library.tar.gz\")\n",
        "\n",
        "# unzip the compressed R library file: 'library.tar.gz' into the R library folder\n",
        "untar(\"library.tar.gz\", \"library\")\n",
        "\n",
        "# change the R library directory into './library'\n",
        ".libPaths(\"library\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfOOPumc0qTp"
      },
      "outputs": [],
      "source": [
        "# We activate relevant packages\n",
        "library(\"dplyr\")\n",
        "library(\"quanteda\")\n",
        "library(\"RCurl\")\n",
        "library(\"quanteda.textplots\")\n",
        "library(\"quanteda.textstats\")\n",
        "library(\"udpipe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Getting Text into R/Python\n"
      ],
      "metadata": {
        "id": "LRVXm0Sr10aR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We load data (a csv-file with ratings and content of TV series) from the Github repository\n",
        "url = getURL(\"https://raw.githubusercontent.com/valeriehase/Salamanca-CSS-SummerSchool/main/Processing%20text%20and%20text%20as%20data/data_tvseries.csv\")\n",
        "data = read.csv2(text = url)"
      ],
      "metadata": {
        "id": "aHh51GXi6KGr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check data by inspecting first rows via head()\n",
        "head(data)"
      ],
      "metadata": {
        "id": "IRsKIIE4Bbb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect weird data in variable \"Year\" for first observation\n",
        "data %>%\n",
        "  select(Year) %>%\n",
        "  slice(1)"
      ],
      "metadata": {
        "id": "W9D_Y7QaCaTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Cleaning/Normalizing Text\n",
        "\n"
      ],
      "metadata": {
        "id": "zRjzaX8lhRbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning Text via Regular Expressions\n",
        "\n"
      ],
      "metadata": {
        "id": "jomUJqMumBbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's remove the number, point and blank space before the TV series in our\n",
        "#variable \"Title\" using gsub()\n",
        "data = data %>%\n",
        "  mutate(Title = gsub(\"^[0-9]+[[:punct:]] \", \"\", Title))\n",
        "\n",
        "#Inspect the first five rows of the resulting data frame\n",
        "data %>%\n",
        "  head(5)"
      ],
      "metadata": {
        "id": "ZZA0P0Os3cyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ok, let's have some fun with this.\n",
        "# Using the grepl() function, we find all TV series that contain the word \"drama\" in the variable \"Description\".\n",
        "# We use filter() to identify these observations.\n",
        "data %>%\n",
        "\n",
        "  #filter all observations containing the word \"drama\"\n",
        "  filter(grepl(\"[D|d]rama\", Description)) %>%\n",
        "\n",
        "  # see first 5 rows of data set\n",
        "  head(5)"
      ],
      "metadata": {
        "id": "lxBGnFU1pnS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's get all observations that contain the word\n",
        "# \"drama\" or the word \"crime\" in the variable \"Description\"\n",
        "data %>%\n",
        "\n",
        "  #filter all observations containing the word \"drama\"\n",
        "  filter(grepl(\"[D|d]rama|[C|c]rime\", Description)) %>%\n",
        "\n",
        "  # see first rows of data set\n",
        "  head(5)"
      ],
      "metadata": {
        "id": "O7ADybrS3llp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your turn!\n",
        "# Can you identify all series that play in Spain?"
      ],
      "metadata": {
        "id": "_vdLUI25-8m5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your turn!\n",
        "# Can you identify all series that deal with superheroes\n",
        "# and replace the term \"superhero/superheroes in the variable \"Description\"\n",
        "# with \"fancy R programmers\"?"
      ],
      "metadata": {
        "id": "YwiCwxXF_llE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizing text\n",
        "\n"
      ],
      "metadata": {
        "id": "UR_NYCBuEw4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Run preprocessing steps using tokens() and subfunctions\n",
        "tokens <- tokens(data$Description,\n",
        "                 what = \"word\",\n",
        "                 remove_punct = TRUE,\n",
        "                 remove_numbers = TRUE) %>%\n",
        "          tokens_tolower() %>%\n",
        "          tokens_remove(stopwords(\"english\")) %>%\n",
        "          tokens_wordstem()"
      ],
      "metadata": {
        "id": "fGrgArBFFTbF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Look at original first text\n",
        "data$Description[1]"
      ],
      "metadata": {
        "id": "KTNDiR1TOg4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Look at preprocessed first text\n",
        "tokens[1]"
      ],
      "metadata": {
        "id": "ynnqFsoAOrs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your turn!\n",
        "# Can you create a list of 3-5 stop words that you think are unique to this corpus\n",
        "# and remove these as part of the existing preprocessing pipeline?"
      ],
      "metadata": {
        "id": "QyrfiyNqMc2U"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text-as-Data Representations\n",
        "\n"
      ],
      "metadata": {
        "id": "hIdETE9Qf9B6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag-of-words approach: Document-feature matrix"
      ],
      "metadata": {
        "id": "JGrXU340gGLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a document-feature matrix\n",
        "dfm = tokens %>%\n",
        "  dfm()\n",
        "\n",
        "#check result\n",
        "dfm"
      ],
      "metadata": {
        "id": "C-YUXhdjgKPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check most frequent features\n",
        "topfeatures = topfeatures(dfm, 10) %>%\n",
        "  as.data.frame() %>%\n",
        "  rename(\"count\" = '.')\n",
        "\n",
        "topfeatures"
      ],
      "metadata": {
        "id": "9vLNlsQii-jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize results with a word cloud\n",
        "textplot_wordcloud(dfm, max_words = 100)"
      ],
      "metadata": {
        "id": "eCHCZAJPhP_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beyond bag-of-words: Ngrams"
      ],
      "metadata": {
        "id": "y8kcemw7uZpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get most frequent collocations\n",
        "tokens %>%\n",
        "  textstat_collocations(min_count = 10) %>%\n",
        "  arrange(-lambda) %>%\n",
        "  head(10)"
      ],
      "metadata": {
        "id": "Ho6PfXkSuYnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beyond bag-of-words: Part-of-speech tagging"
      ],
      "metadata": {
        "id": "N4u551k31Cbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data$Description %>%\n",
        "\n",
        "#change format for udpipe package\n",
        "  as_tibble() %>%\n",
        "  mutate(doc_id = paste0(\"text\", 1:n())) %>%\n",
        "  rename(text = value) %>%\n",
        "\n",
        "  #for simplicity, run for fewer documents\n",
        "  slice(1) %>%\n",
        "\n",
        "  #part-of-speech tagging, include only related variables\n",
        "  udpipe(\"english\") %>%\n",
        "  select(doc_id, sentence_id, token_id, token, upos) %>%\n",
        "  head(10)"
      ],
      "metadata": {
        "id": "ZgVq1WQU1Fhv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}